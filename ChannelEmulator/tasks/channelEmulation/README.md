## Channel Emulation Sequence-to-Sequence Task

### Overview

This task wires a Temporal Convolutional Network (TCN) to emulate a channel response in the time domain. Each training example is a pair of aligned waveforms stored as WAV files: one waveform represents the channel input and the other captures the desired channel output. The model is trained in a sequence-to-sequence regression setup where every audio frame is mapped to its corresponding target frame.

### Data Layout

Place your audio pairs under:

- `tasks/channelEmulation/data/input/*.wav` - dry or clean waveforms generated by `audioGenerator.py`.
- `tasks/channelEmulation/data/output/*.wav` - processed waveforms generated by `audioFX.py`.

The loader matches explicit filenames (defaults to `rawAudio.wav` -> `fxAudioVFuzz.wav`), enforces a consistent sample rate, and slices each waveform into fixed-length windows using a configurable hop size. All waveforms are min-max normalised to the `[0, 1]` range before training. Windows can be limited to keep memory usage manageable.

If no aligned WAV pairs are available the task generates a synthetic dataset (identity channel perturbed with noise) so that the pipeline can still be executed end to end.

### Running the Task

From the `tasks/channelEmulation` folder:

```bash
python main.py --config wavenet3.json --input rawAudio.wav --output fxAudioVFuzz.wav
```

The JSON configuration supplies the sequence length, dilation pattern, overlap ratio, and optimisation settings; the input/output filenames are provided explicitly on the command line. Adjust hyperparameters inside the JSON as needed. When validation windows are available the script enables early stopping. Each trained model is stored automatically in `tasks/channelEmulation/savedModels/<config-name>.keras` so the exported artifact clearly matches the configuration used.

### Inference

To render processed audio with a trained model:

```bash
python inference/inferenceTest.py --model wavenet3.keras --input rawAudio.wav --output fxAudioVFuzz_pred.wav
```

The script loads the config with the same base name as the model, applies the TCN frame by frame, and writes the reconstructed waveform to `tasks/channelEmulation/data/results/<model-name>/<input>_inf.wav` (the `--output` name is ignored in favor of this convention).

### Notes

- The script uses TensorFlow's native WAV decoder, so additional audio dependencies are not required.
- Input and target WAV files must be mono, share the same sample rate, and contain identical numbers of samples—mismatches raise an error early.
- The `overlap` field in each config expresses the fraction of window overlap (for example, `0.5` yields 50% overlap, i.e. half-step hops).
- Review `main.py` for the available command-line flags and for an example of how to adapt the architecture for other channel lengths or feature counts.
